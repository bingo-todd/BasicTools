{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T08:23:44.408389Z",
     "start_time": "2018-12-05T08:23:42.238543Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "? tf.FIFOQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T08:23:44.416926Z",
     "start_time": "2018-12-05T08:23:44.412620Z"
    }
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T08:23:44.550924Z",
     "start_time": "2018-12-05T08:23:44.427670Z"
    }
   },
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T08:25:16.799716Z",
     "start_time": "2018-12-05T08:25:16.466955Z"
    }
   },
   "outputs": [],
   "source": [
    "with file('Data_set/Develop/0.dat','r') as file_obj:\n",
    "    a = cPickle.load(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T08:25:23.560448Z",
     "start_time": "2018-12-05T08:25:23.553834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186, 1771)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.DType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T08:50:47.264515Z",
     "start_time": "2019-06-24T08:50:47.261030Z"
    }
   },
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T08:50:47.421029Z",
     "start_time": "2019-06-24T08:50:47.417177Z"
    }
   },
   "outputs": [],
   "source": [
    "b = [4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T08:50:47.628994Z",
     "start_time": "2019-06-24T08:50:47.625608Z"
    }
   },
   "outputs": [],
   "source": [
    "c= a.extend(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T08:50:51.333135Z",
     "start_time": "2019-06-24T08:50:51.327752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T08:48:56.639956Z",
     "start_time": "2019-06-24T08:48:56.634950Z"
    }
   },
   "outputs": [],
   "source": [
    "a = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T10:44:41.010759Z",
     "start_time": "2019-07-23T10:44:41.004840Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TFData.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile TFData.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pickle\n",
    "import threading\n",
    "\n",
    "class TFData(object):\n",
    "    \"\"\"create a thread to read data when needed\n",
    "    example:\n",
    "\n",
    "    coord = tf.train.Coordinator()# \n",
    "    reader = DataReader('Data_set/Develop/',\n",
    "                        x_shape=[1771],y_len=161,\n",
    "                        batch_size=100,\n",
    "                        coord=coord)    \n",
    "    x_batch,y_batch = reader.dequeue()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "\n",
    "    reader.start_thread()\n",
    "\n",
    "    try:\n",
    "        for i in xrange(100):\n",
    "            x_batch_value,y_batch_value = sess.run([x_batch,y_batch])\n",
    "            print x_batch_value.shape,y_batch_value.shape\n",
    "    except KeyboardInterrupt:\n",
    "        print()\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self,file_dir,x_shape,y_shape,batch_size,batch_num_queue,coord,data_reader=None,is_loop=True):\n",
    "        \"\"\" \n",
    "        pipeline for data feed to neural network\n",
    "        Inputs: \n",
    "            file_dir: data directory\n",
    "            x_shape: list, shape of features, first element should be None\n",
    "            y_shape: list, shape of label, first element should be None\n",
    "            batch_size: batch_size\n",
    "            batch_num_queue: queue length in batchs\n",
    "            coord: \n",
    "            data_reader: function to load data from file, build in function will be used if None\n",
    "            is_loop: bool, if Ture, read all data repeatly, else read all data one once\n",
    "        \"\"\"\n",
    "        \n",
    "        self.is_loop = is_loop\n",
    "        # get all data file_paths, shuffle(approximate data shuffle) and repet(epoch)\n",
    "        self.file_dir = file_dir\n",
    "        # \n",
    "        self.coord = coord\n",
    "        #\n",
    "        self.x_shape = x_shape\n",
    "        self.y_shape = y_shape\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        # data queue\n",
    "        self.x_queue = tf.FIFOQueue(capacity=batch_size*batch_num_queue,\n",
    "                               dtypes=tf.float32,\n",
    "                               shapes=self.x_shape[1:])\n",
    "        self.y_queue = tf.FIFOQueue(capacity=batch_size*batch_num_queue,\n",
    "                               dtypes=tf.float32,\n",
    "                               shapes=self.y_shape[1:])\n",
    "        \n",
    "        # enqueue op\n",
    "        self.x_placeholder = tf.placeholder(dtype=tf.float32,shape=self.x_shape)\n",
    "        self.y_placeholder = tf.placeholder(dtype=tf.float32,shape=self.y_shape)\n",
    "\n",
    "        self.x_enqueue = self.x_queue.enqueue_many([self.x_placeholder])\n",
    "        self.y_enqueue = self.y_queue.enqueue_many([self.y_placeholder])\n",
    "        \n",
    "        # \n",
    "        self.threads = []\n",
    "        \n",
    "        if data_reader == None:\n",
    "            self.data_reader = self._data_reader\n",
    "        else:\n",
    "            self.data_reader = data_reader\n",
    "\n",
    "        self.is_epoch_finish = False\n",
    "            \n",
    "        #\n",
    "    \n",
    "    def dequeue(self):\n",
    "        x_batch = self.x_queue.dequeue_many(self.batch_size)\n",
    "        y_batch = self.y_queue.dequeue_many(self.batch_size)\n",
    "        return [x_batch,y_batch]\n",
    "\n",
    "    def empty_queue(self,sess):\n",
    "        sess.run(self.x_queue.dequeue_many(self.x_queue.size()))\n",
    "        sess.run(self.y_queue.dequeue_many(self.y_queue.size()))\n",
    "    \n",
    "        \n",
    "    def _data_reader(self,file_dir):\n",
    "        \"\"\"read dat file in which both x and y are saved with cPickle\"\"\"\n",
    "        filename_list = os.listdir(self.file_dir)\n",
    "        # screen data\n",
    "        filename_filter = lambda filename: len(filename)>3 and filename[-3:]=='dat' and filename[0] !='.'\n",
    "        filename_list = filter(filename_filter,filename_list)\n",
    "        # repeate filename_list to enable multiple epoch\n",
    "        # shuffle filename_list\n",
    "        np.random.shuffle(filename_list)\n",
    "\n",
    "        for filename in filename_list:\n",
    "            filepath =  os.path.join(self.file_dir,filename)\n",
    "            with file(filepath,'r') as data_file:\n",
    "                x,y = pickle.load(data_file)\n",
    "            yield [x,y]\n",
    "                \n",
    "        \n",
    "        \n",
    "    def _reader_main(self,sess):\n",
    "        \"\"\"\"\"\"\n",
    "        stop = False\n",
    "        while not stop:\n",
    "            # loop until main prosess finish\n",
    "            if self.coord.should_stop(): # train finish, terminate file read\n",
    "                    stop = True\n",
    "                    break\n",
    "            try:\n",
    "                iterator = self.data_reader(self.file_dir)\n",
    "            except:\n",
    "                stop = True\n",
    "                self.coord.request_stop()\n",
    "                \n",
    "            for x_file,y_file in iterator:\n",
    "                if self.coord.should_stop(): # train finish, terminate file read\n",
    "                    stop = True\n",
    "                    break\n",
    "                    \n",
    "                sess.run([self.x_enqueue,self.y_enqueue],\n",
    "                         feed_dict={self.x_placeholder:x_file,self.y_placeholder:y_file})\n",
    "\n",
    "            if not self.is_loop:\n",
    "                self.is_epoch_finish = True\n",
    "                stop = True\n",
    "                \n",
    "    def start_thread(self,sess,n_thread=1):\n",
    "        self.is_epoch_finish = False\n",
    "        for _ in np.arange(n_thread):\n",
    "            thread = threading.Thread(target=self._reader_main,args=(sess,))\n",
    "            thread.daemon = True\n",
    "            thread.start()\n",
    "            self.threads.append(thread)\n",
    "        return self.threads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T08:25:53.835353Z",
     "start_time": "2018-12-05T08:25:53.827148Z"
    }
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "reader = DataReader('Data_set/Develop/',x_len=1771,y_len=161,batch_size=100,coord=coord)    \n",
    "\n",
    "x_batch,y_batch = reader.dequeue()\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "\n",
    "reader.start_thread()\n",
    "\n",
    "try:\n",
    "    for i in xrange(100):\n",
    "        x_batch_value,y_batch_value = sess.run([x_batch,y_batch])\n",
    "        print x_batch_value.shape,y_batch_value.shape\n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "finally:\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st_python3",
   "language": "python",
   "name": "st_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
